var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"\\thispagestyle{empty}","category":"page"},{"location":"references/","page":"References","title":"References","text":"I. Goodfellow, Y. Bengio and A. Courville. Deep learning (MIT press, Cambridge, MA, 2016).\n\n\n\nE. Hairer, C. Lubich and G. Wanner. Geometric Numerical integration: structure-preserving algorithms for ordinary differential equations (Springer, Heidelberg, 2006).\n\n\n\nF. Mezzadri. How to generate random matrices from the classical compact groups, arXiv preprint math-ph/0609050 (2006).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Riemannian geometry of Grassmann manifolds with a view on algorithmic computation. Acta Applicandae Mathematica 80, 199–220 (2004).\n\n\n\nP.-A. Absil, R. Mahony and R. Sepulchre. Optimization algorithms on matrix manifolds (Princeton University Press, Princeton, New Jersey, 2008).\n\n\n\nT. Bendokat, R. Zimmermann and P.-A. Absil. A Grassmann manifold handbook: Basic geometry and computational aspects, arXiv preprint arXiv:2011.13699 (2020).\n\n\n\nT. Bendokat and R. Zimmermann. The real symplectic Stiefel and Grassmann manifolds: metrics, geodesics and applications, arXiv preprint arXiv:2108.12447 (2021).\n\n\n\nB. Brantner. Generalizing Adam To Manifolds For Efficiently Training Transformers, arXiv preprint arXiv:2305.16901 (2023).\n\n\n\n","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GeometricOptimizers","category":"page"},{"location":"#GeometricOptimizers","page":"Home","title":"GeometricOptimizers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for GeometricOptimizers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [GeometricOptimizers]","category":"page"},{"location":"#Base.Matrix-Tuple{GlobalSection}","page":"Home","title":"Base.Matrix","text":"Matrix(λY::GlobalSection)\n\nPut λY into matrix form. \n\nThis is not recommended if speed is important!\n\nUse apply_section and global_rep instead!\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.AbstractCache","page":"Home","title":"GeometricOptimizers.AbstractCache","text":"AbstractCache\n\nAbstractCache has subtypes: AdamCache, MomentumCache, GradientCache and BFGSCache.\n\nAll of them can be initialized with providing an array (also supporting manifold types).\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.AbstractLieAlgHorMatrix","page":"Home","title":"GeometricOptimizers.AbstractLieAlgHorMatrix","text":"AbstractLieAlgHorMatrix <: AbstractMatrix\n\nAbstractLieAlgHorMatrix is a supertype for various horizontal components of Lie algebras. We usually call this mathfrakg^mathrmhor.\n\nSee StiefelLieAlgHorMatrix and GrassmannLieAlgHorMatrix for concrete examples.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.AbstractTriangular","page":"Home","title":"GeometricOptimizers.AbstractTriangular","text":"AbstractTriangular\n\nSee UpperTriangular and LowerTriangular.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.Adam","page":"Home","title":"GeometricOptimizers.Adam","text":"Adam(η, ρ₁, ρ₂, δ)\n\nMake an instance of the Adam Optimizer.\n\nHere the cache consists of first and second moments that are updated as \n\nB_1 gets ((rho_1 - rho_1^t)(1 - rho_1^t))cdotB_1 + (1 - rho_1)(1 - rho_1^t)cdotnablaL\n\nand\n\nB_2 gets ((rho_2 - rho_1^t)(1 - rho_2^t))cdotB_2 + (1 - rho_2)(1 - rho_2^t)cdotnablaLodotnablaL\n\nThe final velocity is computed as:\n\nmathrmvelocity gets -etaB_1sqrtB_2 + delta\n\nImplementation\n\nThe velocity is stored in the input to save memory:\n\nmul!(B, -o.method.η, /ᵉˡᵉ(C.B₁, scalar_add(racᵉˡᵉ(C.B₂), o.method.δ)))\n\nwhere B is the input to the [update!] function.\n\nThe algorithm and suggested defaults are taken from [1, page 301].\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.AdamCache","page":"Home","title":"GeometricOptimizers.AdamCache","text":"AdamCache(Y)\n\nStore the first and second moment for Y (initialized as zeros).\n\nFirst and second moments are called B₁ and B₂.\n\nIf the cache is called with an instance of a homogeneous space, e.g. the StiefelManifold St(nN) it initializes the moments as elements of mathfrakg^mathrmhor (StiefelLieAlgHorMatrix).\n\nExamples\n\nusing GeometricOptimizers\n\nY = rand(StiefelManifold, 5, 3)\nGeometricOptimizers.AdamCache(Y).B₁\n\n# output\n\n5×5 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:\n 0.0  -0.0  -0.0  -0.0  -0.0\n 0.0   0.0  -0.0  -0.0  -0.0\n 0.0   0.0   0.0  -0.0  -0.0\n 0.0   0.0   0.0   0.0   0.0\n 0.0   0.0   0.0   0.0   0.0\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.AdamWithDecay","page":"Home","title":"GeometricOptimizers.AdamWithDecay","text":"AdamWithDecay(n_epochs, η₁=1f-2, η₂=1f-6, ρ₁=9f-1, ρ₂=9.9f-1, δ=1f-8)\n\nMake an instance of the Adam Optimizer with weight decay.\n\nAll except the first argument (the number of epochs) have defaults.\n\nThe difference to the standard Adam is that we change the learning reate eta in each step. Apart from the time dependency of eta the two algorithms are however equivalent. eta(0) starts with a high value eta_1 and then exponentially decrease until it reaches eta_2 with\n\n eta(t) = gamma^teta_1\n\nwhere gamma = exp(log(eta_1  eta_2)  mathttn_epochs)\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.BFGS","page":"Home","title":"GeometricOptimizers.BFGS","text":"BFGS(η, δ)\n\nMake an instance of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) optimizer. \n\nη is the learning rate. δ is a stabilization parameter.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.BFGSCache","page":"Home","title":"GeometricOptimizers.BFGSCache","text":"BFGSCache(B)\n\nMake the cache for the BFGS optimizer based on the array B.\n\nIt stores an array for the gradient of the previous time step B and the inverse of the Hessian matrix H.\n\nThe cache for the inverse of the Hessian is initialized with the idendity. The cache for the previous gradient information is initialized with the zero vector.\n\nNote that the cache for H is changed iteratively, whereas the cache for B is newly assigned at every time step.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.GlobalSection","page":"Home","title":"GeometricOptimizers.GlobalSection","text":"GlobalSection(Y)\n\nConstruct a global section for Y.  \n\nA global section lambda is a mapping from a homogeneous space mathcalM to the corresponding Lie group G such that \n\nlambda(Y)E = Y\n\nAlso see apply_section and global_rep.\n\nImplementation\n\nFor an implementation of GlobalSection for a custom array (especially manifolds), the function global_section has to be generalized.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.Gradient","page":"Home","title":"GeometricOptimizers.Gradient","text":"Gradient(η)\n\nMake an instance of a gradient optimizer. \n\nThis is the simplest neural network optimizer. It has no cache and computes the final velocity as:\n\n    mathrmvelocity gets - etanabla_mathrmweightL\n\nImplementation\n\nThe operations are done as memory efficiently as possible. This means the provided nabla_WL is mutated via:\n\nrmul!(∇L, -method.η)\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.GradientCache","page":"Home","title":"GeometricOptimizers.GradientCache","text":"GradientCache(Y)\n\nDo not store anything.\n\nThe cache for the Gradient does not consider past information.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.GrassmannLieAlgHorMatrix","page":"Home","title":"GeometricOptimizers.GrassmannLieAlgHorMatrix","text":"GrassmannLieAlgHorMatrix(B::AbstractMatrix, N::Integer, n::Integer)\n\nBuild an instance of GrassmannLieAlgHorMatrix based on an arbitrary matrix B of size (N-n)timesn.\n\nGrassmannLieAlgHorMatrix is the horizontal component of the Lie algebra of skew-symmetric matrices (with respect to the canonical metric).\n\nExtended help\n\nThe projection here is: piS to SEsim where \n\nE = beginbmatrix mathbbI_n  mathbbO_(N-n)timesn  endbmatrix\n\nand the equivalence relation is \n\nV_1 sim V_2 iff exists AinmathcalS_mathrmskew(n) text such that  V_2 = V_1 + beginbmatrix A  mathbbO endbmatrix\n\nAn element of GrassmannLieAlgMatrix takes the form: \n\nbeginpmatrix\nbarmathbbO  B^T  B  mathbbO\nendpmatrix\n\nwhere barmathbbOinmathbbR^ntimesn and mathbbOinmathbbR^(N - n)times(N-n)\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.GrassmannLieAlgHorMatrix-Tuple{AbstractMatrix, Int64}","page":"Home","title":"GeometricOptimizers.GrassmannLieAlgHorMatrix","text":"GrassmannLieAlgHorMatrix(D::AbstractMatrix, n::Integer)\n\nTake a big matrix as input and build an instance of GrassmannLieAlgHorMatrix.\n\nThe integer N in Gr(n N) here is the number of rows of D.\n\nExtended help\n\nIf the constructor is called with a big NtimesN matrix, then the projection is performed the following way: \n\nbeginpmatrix\nA  B_1  \nB_2  D\nendpmatrix mapsto \nbeginpmatrix\nbarmathbbO  -B_2^T  \nB_2  mathbbO\nendpmatrix\n\nThis can also be seen as the operation:\n\nD mapsto Omega(E DE - EE^TDE)\n\nwhere Omega is the horizontal lift GeometricOptimizers.Ω.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.GrassmannManifold","page":"Home","title":"GeometricOptimizers.GrassmannManifold","text":"GrassmannManifold <: Manifold\n\nThe GrassmannManifold is based on the StiefelManifold.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.LowerTriangular","page":"Home","title":"GeometricOptimizers.LowerTriangular","text":"LowerTriangular(S::AbstractVector, n::Int)\n\nBuild a lower-triangular matrix from a vector.\n\nA lower-triangular matrix is an ntimesn matrix that has zeros on the diagonal and on the upper triangular.\n\nThe data are stored in a vector S similarly to other matrices. See UpperTriangular, SkewSymMatrix and SymmetricMatrix.\n\nThe struct two fields: S and n. The first stores all the entries of the matrix in a sparse fashion (in a vector) and the second is the dimension n for AinmathbbR^ntimesn.\n\nExamples\n\nusing GeometricOptimizers\nS = [1, 2, 3, 4, 5, 6]\nLowerTriangular(S, 4)\n\n# output\n\n4×4 LowerTriangular{Int64, Vector{Int64}}:\n 0  0  0  0\n 1  0  0  0\n 2  3  0  0\n 4  5  6  0\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.LowerTriangular-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.LowerTriangular","text":"LowerTriangular(A::AbstractMatrix)\n\nBuild a lower-triangular matrix from a matrix.\n\nThis is done by taking the lower left of that matrix.\n\nExamples\n\nusing GeometricOptimizers\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nLowerTriangular(M)\n\n# output\n\n4×4 LowerTriangular{Int64, Vector{Int64}}:\n  0   0   0  0\n  5   0   0  0\n  9  10   0  0\n 13  14  15  0\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.Manifold","page":"Home","title":"GeometricOptimizers.Manifold","text":"Manifold <: AbstractMatrix\n\nA manifold in GeometricOptimizers is a sutype of AbstractMatrix. All manifolds are matrix manifolds and therefore stored as matrices. More details can be found in the docstrings for the StiefelManifold and the GrassmannManifold.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.Momentum","page":"Home","title":"GeometricOptimizers.Momentum","text":"Momentum(η, α)\n\nMake an instance of the momentum optimizer.\n\nThe momentum optimizer is similar to the Gradient. It however has a nontrivial cache that stores past history (see MomentumCache). The cache is updated via:\n\n    B^mathrmcache gets alphaB^mathrmcache + nabla_mathrmweightsL\n\nand then the final velocity is computed as\n\n    mathrmvelocity gets  - etaB^mathrmcache\n\nImplementation\n\nTo save memory the velocity is stored in the input nabla_WL. This is similar to the case of the Gradient.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.MomentumCache","page":"Home","title":"GeometricOptimizers.MomentumCache","text":"MomentumCache(Y)\n\nStore the moment for Y (initialized as zeros).\n\nThe moment is called B.\n\nIf the cache is called with an instance of a Manifold it initializes the moments as elements of mathfrakg^mathrmhor (AbstractLieAlgHorMatrix).\n\nSee AdamCache.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.Optimizer","page":"Home","title":"GeometricOptimizers.Optimizer","text":"Optimizer(method, cache, step, retraction)\n\nStore the method (e.g. Adam with corresponding hyperparameters), the cache (e.g. AdamCache), the optimization step and the retraction.\n\nIt takes as input an optimization method and the parameters of a network. \n\nBefore one can call Optimizer a OptimizerMethod that stores all the hyperparameters of the optimizer needs to be specified. \n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.Optimizer-Tuple{GeometricOptimizers.OptimizerMethod, NamedTuple}","page":"Home","title":"GeometricOptimizers.Optimizer","text":"Optimizer(method, nn_params)\n\nAllocate the cache for a specific method and nn_params for an instance of Optimizer.\n\nInternally this calls init_optimizer_cache.\n\nAn equivalent constructor is\n\nOptimizer(method, nn::NeuralNetwork)\n\nArguments\n\nThe optional keyword argument is the retraction. By default this is cayley.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.OptimizerMethod","page":"Home","title":"GeometricOptimizers.OptimizerMethod","text":"OptimizerMethod\n\nEach Optimizer has to be called with an OptimizerMethod. This specifies how the neural network weights are updated in each optimization step.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.SkewSymMatrix","page":"Home","title":"GeometricOptimizers.SkewSymMatrix","text":"SkewSymMatrix(S::AbstractVector, n::Integer)\n\nInstantiate a skew-symmetric matrix with information stored in vector S.\n\nA skew-symmetric matrix A is a matrix A^T = -A.\n\nInternally the struct saves a vector S of size n(n-1)div2. The conversion is done the following way: \n\nA_ij = begincases 0                              textif i=j \n                         S( (i-2) (i-1) ) div 2 + j  textif ij \n                         S( (j-2) (j-1) ) div 2 + i  textelse endcases\n\nSo S stores a string of vectors taken from A: S = tildea_1 tildea_2 ldots tildea_n with tildea_i = A_i1A_i2ldotsA_i(i-1).\n\nAlso see SymmetricMatrix, LowerTriangular and UpperTriangular.\n\nExamples\n\nusing GeometricOptimizers\nS = [1, 2, 3, 4, 5, 6]\nSkewSymMatrix(S, 4)\n\n# output\n\n4×4 SkewSymMatrix{Int64, Vector{Int64}}:\n 0  -1  -2  -4\n 1   0  -3  -5\n 2   3   0  -6\n 4   5   6   0\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.SkewSymMatrix-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.SkewSymMatrix","text":"SkewSymMatrix(A::AbstractMatrix)\n\nPerform 0.5 * (A - A') and store the matrix in an efficient way (as a vector with n(n-1)2 entries).\n\nIf the constructor is called with a matrix as input it returns a skew-symmetric matrix via the projection:\n\nA mapsto frac12(A - A^T)\n\nExamples\n\nusing GeometricOptimizers\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSkewSymMatrix(M)\n\n# output\n\n4×4 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -1.5  -3.0  -4.5\n 1.5   0.0  -1.5  -3.0\n 3.0   1.5   0.0  -1.5\n 4.5   3.0   1.5   0.0\n\nExtended help\n\nNote that the constructor is designed in such a way that it always returns matrices of type SkewSymMatrix{<:AbstractFloat} when called with a matrix, even if this matrix is of type AbstractMatrix{<:Integer}.\n\nIf the user wishes to allocate a matrix SkewSymMatrix{<:Integer} then call:\n\nSkewSymMatrix(::AbstractVector, n::Integer)\n\nNote that this is different from LowerTriangular and UpperTriangular as no porjection takes place there.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.StiefelLieAlgHorMatrix","page":"Home","title":"GeometricOptimizers.StiefelLieAlgHorMatrix","text":"StiefelLieAlgHorMatrix(A::SkewSymMatrix, B::AbstractMatrix, N::Integer, n::Integer)\n\nBuild an instance of StiefelLieAlgHorMatrix based on a skew-symmetric matrix A and an arbitrary matrix B.\n\nAn element of StiefelLieAlgMatrix takes the form: \n\nbeginpmatrix\nA  B^T  B  mathbbO\nendpmatrix\n\nwhere A is skew-symmetric (this is SkewSymMatrix in GeometricOptimizers).\n\nAlso see GrassmannLieAlgHorMatrix.\n\nExtended help\n\nStiefelLieAlgHorMatrix is the horizontal component of the Lie algebra of skew-symmetric matrices (with respect to the canonical metric).\n\nThe projection here is: piS to SE where \n\nE = beginbmatrix mathbbI_n  mathbbO_(N-n)timesn  endbmatrix\n\nThe matrix E is implemented under StiefelProjection in GeometricOptimizers.\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.StiefelLieAlgHorMatrix-Tuple{AbstractMatrix, Integer}","page":"Home","title":"GeometricOptimizers.StiefelLieAlgHorMatrix","text":"StiefelLieAlgHorMatrix(D::AbstractMatrix, n::Integer)\n\nTake a big matrix as input and build an instance of StiefelLieAlgHorMatrix.\n\nThe integer N in St(n N) is the number of rows of D.\n\nExtended help\n\nIf the constructor is called with a big NtimesN matrix, then the projection is performed the following way: \n\nbeginpmatrix\nA  B_1  \nB_2  D\nendpmatrix mapsto \nbeginpmatrix\nmathrmskew(A)  -B_2^T  \nB_2  mathbbO\nendpmatrix\n\nThe operation mathrmskewmathbbR^ntimesntomathcalS_mathrmskew(n) is the skew-symmetrization operation. This is equivalent to calling of SkewSymMatrix with an ntimesn matrix.\n\nThis can also be seen as the operation:\n\nD mapsto Omega(E DE) = mathrmskewleft(2 left(mathbbI - frac12 E E^T right) DE E^Tright)\n\nAlso see GeometricOptimizers.Ω.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.StiefelManifold","page":"Home","title":"GeometricOptimizers.StiefelManifold","text":"StiefelManifold <: Manifold\n\nAn implementation of the Stiefel manifold [2]. The Stiefel manifold is the collection of all matrices YinmathbbR^Ntimesn whose columns are orthonormal, i.e. \n\n    St(n N) = Y Y^TY = mathbbI_n \n\nThe Stiefel manifold can be shown to have manifold structure (as the name suggests) and this is heavily used in GeometricOptimizers. It is further a compact space.  More information can be found in the docstrings for rgrad(::StiefelManifold, ::AbstractMatrix) and metric(::StiefelManifold, ::AbstractMatrix, ::AbstractMatrix).\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.StiefelProjection","page":"Home","title":"GeometricOptimizers.StiefelProjection","text":"StiefelProjection(backend, T, N, n)\n\nMake a matrix of the form beginbmatrix mathbbI  mathbbO endbmatrix^T for a specific backend and data type.\n\nAn array that essentially does vcat(I(n), zeros(N-n, n)) with GPU support. \n\nExtended help\n\nAn instance of StiefelProjection should technically also belong to StiefelManifold. \n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.StiefelProjection-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.StiefelProjection","text":"StiefelProjection(A::AbstractMatrix)\n\nExtract necessary information from A and build an instance of StiefelProjection. \n\nNecessary information here referes to the backend, the data type and the size of the matrix.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.StiefelProjection-Union{Tuple{GeometricOptimizers.AbstractLieAlgHorMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.StiefelProjection","text":"StiefelProjection(B::AbstractLieAlgHorMatrix)\n\nExtract necessary information from B and build an instance of StiefelProjection. \n\nNecessary information here referes to the backend, the data type and the size of the matrix.\n\nThe size is queried through B.N and B.n.\n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: StiefelProjection\n\nB₁ = rand(StiefelLieAlgHorMatrix, 5, 2)\nB₂ = rand(GrassmannLieAlgHorMatrix, 5, 2)\nE = [1. 0.; 0. 1.; 0. 0.; 0. 0.; 0. 0.]\n\nStiefelProjection(B₁) ≈ StiefelProjection(B₂) ≈ E \n\n# output\n\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.SymmetricMatrix","page":"Home","title":"GeometricOptimizers.SymmetricMatrix","text":"SymmetricMatrix(S::AbstractVector, n::Integer)\n\nInstantiate a symmetric matrix with information stored in vector S.\n\nA SymmetricMatrix A is a matrix A^T = A.\n\nInternally the struct saves a vector S of size n(n+1)div2. The conversion is done the following way: \n\nA_ij = begincases S( (i-1) i ) div 2 + j  textif igeqj \n                         S( (j-1) j ) div 2 + i  textelse endcases\n\nSo S stores a string of vectors taken from A: S = tildea_1 tildea_2 ldots tildea_n with tildea_i = A_i1A_i2ldotsA_ii.\n\nAlso see SkewSymMatrix, LowerTriangular and UpperTriangular.\n\nExamples\n\nusing GeometricOptimizers\nS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nSymmetricMatrix(S, 4)\n\n# output\n\n4×4 SymmetricMatrix{Int64, Vector{Int64}}:\n 1  2  4   7\n 2  3  5   8\n 4  5  6   9\n 7  8  9  10\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.SymmetricMatrix-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.SymmetricMatrix","text":"SymmetricMatrix(A::AbstractMatrix)\n\nPerform a projection and store the matrix in an efficient way (as a vector with n(n+1)2 entries).\n\nIf the constructor is called with a matrix as input it returns a symmetric matrix via the projection:\n\nA mapsto frac12(A + A^T)\n\nExamples\n\nusing GeometricOptimizers\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSymmetricMatrix(M)\n\n# output\n\n4×4 SymmetricMatrix{Float64, Vector{Float64}}:\n 1.0   3.5   6.0   8.5\n 3.5   6.0   8.5  11.0\n 6.0   8.5  11.0  13.5\n 8.5  11.0  13.5  16.0\n\nExtended help\n\nNote that the constructor is designed in such a way that it always returns matrices of type SymmetricMatrix{<:AbstractFloat} when called with a matrix, even if this matrix is of type AbstractMatrix{<:Integer}.\n\nIf the user wishes to allocate a matrix SymmetricMatrix{<:Integer} then call\n\njulia SymmetricMatrix(AbstractVector nInteger)`\n\nNote that this is different from LowerTriangular and UpperTriangular as no porjection takes place there.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.UpperTriangular","page":"Home","title":"GeometricOptimizers.UpperTriangular","text":"UpperTriangular(S::AbstractVector, n::Int)\n\nBuild an upper-triangular matrix from a vector.\n\nAn upper-triangular matrix is an ntimesn matrix that has zeros on the diagonal and on the lower triangular.\n\nThe data are stored in a vector S similarly to other matrices. See LowerTriangular, SkewSymMatrix and SymmetricMatrix.\n\nThe struct two fields: S and n. The first stores all the entries of the matrix in a sparse fashion (in a vector) and the second is the dimension n for AinmathbbR^ntimesn.\n\nExamples\n\nusing GeometricOptimizers\nS = [1, 2, 3, 4, 5, 6]\nUpperTriangular(S, 4)\n\n# output\n\n4×4 UpperTriangular{Int64, Vector{Int64}}:\n 0  1  2  4\n 0  0  3  5\n 0  0  0  6\n 0  0  0  0\n\n\n\n\n\n","category":"type"},{"location":"#GeometricOptimizers.UpperTriangular-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.UpperTriangular","text":"UpperTriangular(A::AbstractMatrix)\n\nBuild an upper-triangular matrix from a matrix.\n\nThis is done by taking the upper right of that matrix.\n\nExamples\n\nusing GeometricOptimizers\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nUpperTriangular(M)\n\n# output\n\n4×4 UpperTriangular{Int64, Vector{Int64}}:\n 0  2  3   4\n 0  0  7   8\n 0  0  0  12\n 0  0  0   0\n\n\n\n\n\n","category":"method"},{"location":"#Base.:*-Tuple{GlobalSection, Manifold}","page":"Home","title":"Base.:*","text":"λY * Y\n\nApply the element λY onto Y.\n\nHere λY is an element of a Lie group and Y is an element of a homogeneous space.\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Union{Tuple{MT}, Tuple{KernelAbstractions.Backend, Type{MT}, Integer, Integer}} where MT<:Manifold","page":"Home","title":"Base.rand","text":"rand(backend, manifold_type, N, n)\n\nDraw random elements for a specific device.\n\nExamples\n\nRandom elements of the manifold can be allocated on GPU.  Call ...\n\nrand(CUDABackend(), StiefelManifold{Float32}, N, n)\n\n... for drawing elements on a CUDA device.\n\n\n\n\n\n","category":"method"},{"location":"#Base.rand-Union{Tuple{MT}, Tuple{Type{MT}, Integer, Integer}} where MT<:Manifold","page":"Home","title":"Base.rand","text":"rand(manifold_type, N, n)\n\nDraw random elements from the Stiefel and the Grassmann manifold. \n\nBecause both of these manifolds are compact spaces we can sample them uniformly [3].\n\nExamples\n\nWhen we call ...\n\nusing GeometricOptimizers\nusing GeometricOptimizers: _round # hide\nimport Random\nRandom.seed!(123)\n\nN, n = 5, 3\nY = rand(StiefelManifold{Float32}, N, n)\n_round(Y; digits = 5) # hide\n\n# output\n\n5×3 StiefelManifold{Float32, Matrix{Float32}}:\n -0.27575   0.32991   0.77275\n -0.62485  -0.33224  -0.0686\n -0.69333   0.36724  -0.18988\n -0.09295  -0.73145   0.46064\n  0.2102    0.33301   0.38717\n\n... the sampling is done by first allocating a random matrix of size Ntimesn via Y = randn(Float32, N, n).\n\nWe then perform a QR decomposition Q, R = qr(Y) with the qr function from the LinearAlgebra package (this is using Householder reflections internally). \n\nThe final output are then the first n columns of the Q matrix. \n\n\n\n\n\n","category":"method"},{"location":"#Base.vec-Tuple{GeometricOptimizers.AbstractTriangular}","page":"Home","title":"Base.vec","text":"vec(A::AbstractTriangular)\n\nReturn the associated vector to A.\n\nExamples\n\nusing GeometricOptimizers\n\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nLowerTriangular(M) |> vec\n\n# output\n\n6-element Vector{Int64}:\n  5\n  9\n 10\n 13\n 14\n 15\n\n\n\n\n\n","category":"method"},{"location":"#Base.vec-Tuple{SkewSymMatrix}","page":"Home","title":"Base.vec","text":"vec(A)\n\nOutput the associated vector of A.\n\nExamples\n\nusing GeometricOptimizers\n\nM = [1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16]\nSkewSymMatrix(M) |> vec\n\n# output\n\n6-element Vector{Float64}:\n 1.5\n 3.0\n 1.5\n 4.5\n 3.0\n 1.5\n\n\n\n\n\n","category":"method"},{"location":"#Base.vec-Tuple{StiefelLieAlgHorMatrix}","page":"Home","title":"Base.vec","text":"vec(A::StiefelLieAlgHorMatrix)\n\nVectorize A. \n\nExamples\n\nusing GeometricOptimizers\n\nA = SkewSymMatrix([1, ], 2)\nB = [2 3; ]\nB̄ = StiefelLieAlgHorMatrix(A, B, 3, 2)\nB̄ |> vec\n\n# output\n\nvcat(1-element Vector{Int64}, 2-element Vector{Int64}):\n 1\n 2\n 3\n\nImplementation\n\nThis is using Vcat from the package LazyArrays.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.apply_section!-Union{Tuple{AT}, Tuple{T}, Tuple{AT, GlobalSection{T, AT}, AT}} where {T, AT<:(StiefelManifold{T, AT} where AT<:AbstractMatrix{T})}","page":"Home","title":"GeometricOptimizers.apply_section!","text":"apply_section!(Y::AT, λY::GlobalSection{T, AT}, Y₂::AT) where {T, AT<:StiefelManifold{T}}\n\nApply λY to Y₂ and store the result in Y.\n\nThis is the inplace version of apply_section.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.apply_section-Union{Tuple{AT}, Tuple{T}, Tuple{GlobalSection{T, AT}, AT}} where {T, AT<:(StiefelManifold{T, AT} where AT<:AbstractMatrix{T})}","page":"Home","title":"GeometricOptimizers.apply_section","text":"apply_section(λY::GlobalSection{T, AT}, Y₂::AT) where {T, AT <: StiefelManifold{T}}\n\nApply λY to Y₂.\n\nMathematically this is the group action of the element lambdaYinG on the element Y_2 of the homogeneous space mathcalM.\n\nInternally it calls apply_section!.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.cayley-Tuple{GrassmannLieAlgHorMatrix}","page":"Home","title":"GeometricOptimizers.cayley","text":"cayley(B̄::GrassmannLieAlgHorMatrix)\n\nCompute the Cayley retraction of B.\n\nThis is equivalent to the method of cayley for StiefelLieAlgHorMatrix.\n\nSee cayley(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.cayley-Tuple{StiefelLieAlgHorMatrix}","page":"Home","title":"GeometricOptimizers.cayley","text":"cayley(B̄::StiefelLieAlgHorMatrix)\n\nCompute the Cayley retraction of B.\n\nImplementation\n\nInternally this is using \n\nmathrmCayley(barB) = mathbbI + frac12 B (mathbbI_2n - frac12 (B)^T B)^-1 (B)^T (mathbbI + frac12 B)\n\nwith\n\nbarB = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix = beginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrix = B(B)^T\n\ni.e. barB is expressed as a product of two Ntimes2n matrices.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.cayley-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T","page":"Home","title":"GeometricOptimizers.cayley","text":"cayley(Y::Manifold, Δ)\n\nTake as input an element of a manifold Y and a tangent vector in Δ in the corresponding tangent space and compute the Cayley retraction.\n\nIn different notation: take as input an element x of mathcalM and an element of T_xmathcalM and return mathrmCayley(v_x) \n\nExamples\n\nusing GeometricOptimizers\n\nY = StiefelManifold([1. 0. 0.;]' |> Matrix)\nΔ = [0. .5 0.;]' |> Matrix\nY₂ = GeometricOptimizers.cayley(Y, Δ)\n\nY₂' * Y₂ ≈ [1.;]\n\n# output\n\ntrue\n\nSee the example in [geodesic(::Manifold{T}, ::AbstractMatrix{T}) where T].\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.geodesic-Tuple{GrassmannLieAlgHorMatrix}","page":"Home","title":"GeometricOptimizers.geodesic","text":"geodesic(B̄::GrassmannLieAlgHorMatrix)\n\nCompute the geodesic of an element in GrassmannLieAlgHorMatrix.\n\nThis is equivalent to the method of geodesic for StiefelLieAlgHorMatrix.\n\nSee geodesic(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.geodesic-Tuple{StiefelLieAlgHorMatrix}","page":"Home","title":"GeometricOptimizers.geodesic","text":"geodesic(B̄::StiefelLieAlgHorMatrix)\n\nCompute the geodesic of an element in StiefelLieAlgHorMatrix.\n\nImplementation\n\nInternally this is using:\n\nmathbbI + BmathfrakA(B B)B\n\nwith \n\nbarB = beginbmatrix\n    A  -B^T  \n    B  mathbbO\nendbmatrix = beginbmatrix  frac12A  mathbbI  B  mathbbO endbmatrix beginbmatrix  mathbbI  mathbbO  frac12A  -B^T  endbmatrix = B(B)^T\n\nThis is using a computationally efficient version of the matrix exponential mathfrakA. \n\nSee GeometricOptimizers.𝔄.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.geodesic-Union{Tuple{T}, Tuple{Manifold{T}, AbstractMatrix{T}}} where T","page":"Home","title":"GeometricOptimizers.geodesic","text":"geodesic(Y::Manifold, Δ)\n\nTake as input an element of a manifold Y and a tangent vector in Δ in the corresponding tangent space and compute the geodesic (exponential map).\n\nIn different notation: take as input an element x of mathcalM and an element of T_xmathcalM and return mathttgeodesic(x v_x) = exp(v_x)\n\nExamples\n\nusing GeometricOptimizers\n\nY = StiefelManifold([1. 0. 0.;]' |> Matrix)\nΔ = [0. .5 0.;]' |> Matrix\nY₂ = GeometricOptimizers.geodesic(Y, Δ)\n\nY₂' * Y₂ ≈ [1.;]\n\n# output\n\ntrue\n\nImplementation\n\nInternally this geodesic method calls geodesic(::StiefelLieAlgHorMatrix).\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.global_rep-Union{Tuple{AT}, Tuple{T}, Tuple{GlobalSection{T, AT}, AbstractMatrix{T}}} where {T, AT<:(GrassmannManifold{T, AT} where AT<:AbstractMatrix{T})}","page":"Home","title":"GeometricOptimizers.global_rep","text":"global_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT<:GrassmannManifold{T}}\n\nExpress Δ (an element of the tangent space of Y) as an instance of GrassmannLieAlgHorMatrix.\n\nThe method global_rep for GrassmannManifold is similar to that for StiefelManifold.\n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: _round\nimport Random \n\nRandom.seed!(123)\n\nY = rand(GrassmannManifold, 6, 3)\nΔ = rgrad(Y, randn(6, 3))\nλY = GlobalSection(Y)\n\n_round(global_rep(λY, Δ); digits = 3)\n\n# output\n\n6×6 GrassmannLieAlgHorMatrix{Float64, Matrix{Float64}}:\n  0.0     0.0     0.0     0.981  -2.058   0.4\n  0.0     0.0     0.0    -0.424   0.733  -0.919\n  0.0     0.0     0.0    -1.815   1.409   1.085\n -0.981   0.424   1.815   0.0     0.0     0.0\n  2.058  -0.733  -1.409   0.0     0.0     0.0\n -0.4     0.919  -1.085   0.0     0.0     0.0\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.global_rep-Union{Tuple{AT}, Tuple{T}, Tuple{GlobalSection{T, AT}, AbstractMatrix{T}}} where {T, AT<:(StiefelManifold{T, AT} where AT<:AbstractMatrix{T})}","page":"Home","title":"GeometricOptimizers.global_rep","text":"global_rep(λY::GlobalSection{T, AT}, Δ::AbstractMatrix{T}) where {T, AT<:StiefelManifold{T}}\n\nExpress Δ (an the tangent space of Y) as an instance of StiefelLieAlgHorMatrix.\n\nThis maps an element from T_YmathcalM to an element of mathfrakg^mathrmhor. \n\nThese two spaces are isomorphic where the isomorphism where the isomorphism is established through lambda(Y)inG via:\n\nT_YmathcalM to mathfrakg^mathrmhor Delta mapsto lambda(Y)^-1Omega(Y Delta)lambda(Y)\n\nAlso see GeometricOptimizers.Ω.\n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: _round\nimport Random \n\nRandom.seed!(123)\n\nY = rand(StiefelManifold, 6, 3)\nΔ = rgrad(Y, randn(6, 3))\nλY = GlobalSection(Y)\n\n_round(global_rep(λY, Δ); digits = 3)\n\n# output\n\n6×6 StiefelLieAlgHorMatrix{Float64, SkewSymMatrix{Float64, Vector{Float64}}, Matrix{Float64}}:\n  0.0     0.679   1.925   0.981  -2.058   0.4\n -0.679   0.0     0.298  -0.424   0.733  -0.919\n -1.925  -0.298   0.0    -1.815   1.409   1.085\n -0.981   0.424   1.815   0.0     0.0     0.0\n  2.058  -0.733  -1.409   0.0     0.0     0.0\n -0.4     0.919  -1.085   0.0     0.0     0.0\n\nImplementation\n\nThe function global_rep does in fact not perform the entire map lambda(Y)^-1Omega(Y Delta)lambda(Y) but only\n\nDelta mapsto mathrmskew(Y^TDelta)\n\nto get the small skew-symmetric matrix AinmathcalS_mathrmskew(n) and \n\nDelta mapsto (lambda(Y)_1N nN^T Delta)_1(N-n) 1n\n\nto get the arbitrary matrix BinmathbbR^(N-n)timesn.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.global_section-Union{Tuple{GrassmannManifold{T, AT} where AT<:AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.global_section","text":"global_section(Y::GrassmannManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThe method global_section for the Grassmann manifold is equivalent to that for the StiefelManifold (we represent the Grassmann manifold as an embedding in the Stiefel manifold). \n\nSee the documentation for global_section(Y::StiefelManifold{T}) where T. \n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.global_section-Union{Tuple{StiefelManifold{T, AT} where AT<:AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"GeometricOptimizers.global_section","text":"global_section(Y::StiefelManifold)\n\nCompute a matrix of size Ntimes(N-n) whose columns are orthogonal to the columns in Y.\n\nThis matrix is also called Y_perp [4–6].\n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: global_section\nimport Random\n\nRandom.seed!(123)\n\nY = StiefelManifold([1. 0.; 0. 1.; 0. 0.; 0. 0.])\n\nround.(Matrix(global_section(Y)); digits = 3)\n\n# output\n\n4×2 Matrix{Float64}:\n 0.0    -0.0\n 0.0     0.0\n 0.936  -0.353\n 0.353   0.936\n\nFurther note that we convert the QRCompactWYQ object to a Matrix before we display it.\n\nImplementation\n\nThe implementation is done with a QR decomposition (LinearAlgebra.qr!). Internally we do: \n\nA = randn(N, N - n) # or the gpu equivalent\nA = A - Y.A * (Y.A' * A)\nqr!(A).Q\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.init_optimizer_cache-Tuple{GeometricOptimizers.OptimizerMethod, Any}","page":"Home","title":"GeometricOptimizers.init_optimizer_cache","text":"init_optimizer_cache(method, x)\n\nInitialize the optimizer cache based on input x for the given method.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.metric-Tuple{GrassmannManifold, AbstractMatrix, AbstractMatrix}","page":"Home","title":"GeometricOptimizers.metric","text":"metric(Y::GrassmannManifold, Δ₁::AbstractMatrix, Δ₂::AbstractMatrix)\n\nCompute the metric for vectors Δ₁ and Δ₂ at Y. \n\nThe representation of the Grassmann manifold is realized as a quotient space of the Stiefel manifold. \n\nThe metric for the Grassmann manifold is:\n\ng^Gr_Y(Delta_1 Delta_2) = g^St_Y(Delta_1 Delta_2) = mathrmTr(Delta_1^T (mathbbI - Y Y^T) Delta_2) = mathrmTr(Delta_1^T Delta_2)\n\nwhere we used that Y^TDelta_i for i = 1 2\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.metric-Tuple{StiefelManifold, AbstractMatrix, AbstractMatrix}","page":"Home","title":"GeometricOptimizers.metric","text":"metric(Y::StiefelManifold, Δ₁::AbstractMatrix, Δ₂::AbstractMatrix)\n\nCompute the dot product for Δ₁ and Δ₂ at Y.\n\nThis uses the canonical Riemannian metric for the Stiefel manifold:\n\ng_Y (Delta_1 Delta_2) mapsto mathrmTr(Delta_1^T(mathbbI - frac12YY^T)Delta_2)\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.optimization_step!-Tuple{Optimizer, NamedTuple, NamedTuple, NamedTuple}","page":"Home","title":"GeometricOptimizers.optimization_step!","text":"optimization_step!(o, λY, ps, dx)\n\nUpdate the weights ps based on an Optimizer, a cache and first-order derivatives dx.\n\noptimization_step! is calling update! internally.  update! has to be implemented for every OptimizerMethod.\n\nArguments\n\nAll arguments into optimization_step! are mandatory:\n\no::Optimizer,\nλY::NamedTuple: this named tuple has the same keys as ps, but contains GlobalSections,\nps::NamedTuple: the neural network parameters,\ndx::NamedTuple: the gradients stores as a NamedTuple.\n\nAll the arguments are given as NamedTuples  as the neural network weights are stores in that format.\n\nusing GeometricOptimizers\nusing GeometricOptimizers: MomentumCache, Momentum, apply_toNT, geodesic, optimization_step!\n\nps = (weight = rand(StiefelManifold{Float32}, 5, 3), )\ncache = apply_toNT(MomentumCache, ps)\no = Optimizer(Momentum(), cache, 0, geodesic)\nλY = GlobalSection(ps)\ndx = (weight = rand(Float32, 5, 3), )\n\n# call the optimizer\noptimization_step!(o, λY, ps, dx)\n\n_test_nt(x) = typeof(x) <: NamedTuple\n\n_test_nt(λY) & _test_nt(ps) & _test_nt(cache) & _test_nt(dx)\n\n# output\n\ntrue\n\nExtended help\n\nThe derivatives dx here are usually obtained via an AD routine by differentiating a loss function, i.e. dx is nabla_xL.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.rgrad-Tuple{GrassmannManifold, AbstractMatrix}","page":"Home","title":"GeometricOptimizers.rgrad","text":"rgrad(Y::GrassmannManifold, ∇L::AbstractMatrix)\n\nCompute the Riemannian gradient for the Grassmann manifold at Y based on ∇L.\n\nHere Y is a representation of mathrmspan(Y)inGr(n N) and nablaLinmathbbR^Ntimesn is the Euclidean gradient. \n\nThis gradient has the property that it is orthogonal to the space spanned by Y.\n\nThe precise form of the mapping is: \n\nmathttrgrad(Y nablaL) mapsto nablaL - YY^TnablaL\n\nNote the property Y^Tmathrmrgrad(Y nablaL) = mathbbO\n\nAlso see rgrad(::StiefelManifold, ::AbstractMatrix).\n\nExamples\n\nusing GeometricOptimizers\n\nY = GrassmannManifold([1 0 ; 0 1 ; 0 0; 0 0])\nΔ = [1 2; 3 4; 5 6; 7 8]\nrgrad(Y, Δ)\n\n# output\n\n4×2 Matrix{Int64}:\n 0  0\n 0  0\n 5  6\n 7  8\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.rgrad-Tuple{StiefelManifold, AbstractMatrix}","page":"Home","title":"GeometricOptimizers.rgrad","text":"rgrad(Y::StiefelManifold, ∇L::AbstractMatrix)\n\nCompute the Riemannian gradient for the Stiefel manifold at Y based on ∇L.\n\nHere YinSt(Nn) and nablaLinmathbbR^Ntimesn is the Euclidean gradient. \n\nThe function computes the Riemannian gradient with respect to the canonical metric: metric(::StiefelManifold, ::AbstractMatrix, ::AbstractMatrix).\n\nThe precise form of the mapping is: \n\nmathttrgrad(Y nablaL) mapsto nablaL - Y(nablaL)^TY\n\nNote the property Y^Tmathttrgrad(Y nablaL)inmathcalS_mathrmskew(n)\n\nExamples\n\nusing GeometricOptimizers\n\nY = StiefelManifold([1 0 ; 0 1 ; 0 0; 0 0])\nΔ = [1 2; 3 4; 5 6; 7 8]\nrgrad(Y, Δ)\n\n# output\n\n4×2 Matrix{Int64}:\n 0  -1\n 1   0\n 5   6\n 7   8\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.update!-Tuple{Optimizer, GeometricOptimizers.AbstractCache, AbstractArray}","page":"Home","title":"GeometricOptimizers.update!","text":"update!(o, cache, B)\n\nUpdate the cache and output a final velocity that is stored in B.\n\nNote that Binmathfrakg^mathrmhor in general.\n\nIn the manifold case the final velocity is the input to a retraction.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.update!-Tuple{Optimizer{<:GeometricOptimizers.BFGS}, GeometricOptimizers.BFGSCache, AbstractArray}","page":"Home","title":"GeometricOptimizers.update!","text":"update!(o::Optimizer{<:BFGS}, C, B)\n\nPeform an update with the BFGS optimizer. \n\nC is the cache, B contains the gradient information (the output of global_rep in general).\n\nFirst we compute the final velocity with\n\nvecS = -o.method.η * C.H * vec(B)\n\nand then we update H\n\nC.H .= (𝕀 - ρ * SY) * C.H * (𝕀 - ρ * SY') + ρ * vecS * vecS'\n\nwhere SY is vecS * Y' and 𝕀 is the idendity. \n\nImplementation\n\nFor stability we use δ for computing ρ:\n\nρ = 1. / (vecS' * Y + o.method.δ)\n\nThis is similar to the Adam\n\nExtended help\n\nIf we have weights on a Manifold than the updates are slightly more difficult. In this case the vec operation has to be generalized to the corresponding global tangent space.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.Ω-Union{Tuple{T}, Tuple{GrassmannManifold{T, AT} where AT<:AbstractMatrix{T}, AbstractMatrix{T}}} where T","page":"Home","title":"GeometricOptimizers.Ω","text":"Ω(Y::GrassmannManifold{T}, Δ::AbstractMatrix{T}) where T\n\nPerform the canonical horizontal lift for the Grassmann manifold:\n\n    Delta mapsto Omega^St(Delta)\n\nwhere Omega^St is the canonical horizontal lift for the Stiefel manifold.\n\nusing GeometricOptimizers\nusing GeometricOptimizers: StiefelProjection\n\nE = GrassmannManifold(StiefelProjection(5, 2))\nΔ = [0. 0.; 0. 0.; 2. 3.; 4. 5.; 6. 7.]\nGeometricOptimizers.Ω(E, Δ)\n\n# output\n\n5×5 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -0.0  -2.0  -4.0  -6.0\n 0.0   0.0  -3.0  -5.0  -7.0\n 2.0   3.0   0.0  -0.0  -0.0\n 4.0   5.0   0.0   0.0  -0.0\n 6.0   7.0   0.0   0.0   0.0\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.Ω-Union{Tuple{T}, Tuple{StiefelManifold{T, AT} where AT<:AbstractMatrix{T}, AbstractMatrix{T}}} where T","page":"Home","title":"GeometricOptimizers.Ω","text":"Ω(Y::StiefelManifold{T}, Δ::AbstractMatrix{T}) where T\n\nPerform canonical horizontal lift for the Stiefel manifold:\n\n    Delta mapsto (mathbbI - frac12YY^T)DeltaY^T - YDelta^T(mathbbI - frac12YY^T)\n\nInternally this performs \n\nSkewSymMatrix(2 * (I(n) - .5 * Y * Y') * Δ * Y')\n\nIt uses SkewSymMatrix to save memory. \n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: StiefelProjection\n\nE = StiefelManifold(StiefelProjection(5, 2))\nΔ = [0. -1.; 1. 0.; 2. 3.; 4. 5.; 6. 7.]\nGeometricOptimizers.Ω(E, Δ)\n\n# output\n\n5×5 SkewSymMatrix{Float64, Vector{Float64}}:\n 0.0  -1.0  -2.0  -4.0  -6.0\n 1.0   0.0  -3.0  -5.0  -7.0\n 2.0   3.0   0.0  -0.0  -0.0\n 4.0   5.0   0.0   0.0  -0.0\n 6.0   7.0   0.0   0.0   0.0\n\nNote that the output of Ω is a skew-symmetric matrix, i.e. an element of mathfrakg.\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.𝔄-Tuple{AbstractMatrix, AbstractMatrix}","page":"Home","title":"GeometricOptimizers.𝔄","text":"𝔄(B̂, B̄)\n\nCompute mathfrakA(B B) = sum_n=1^infty frac1n ((B)^TB)^n-1\n\nThis expression has the property mathbbI +  BmathfrakA(B B)(B)^T = exp(B(B)^T)\n\nExamples\n\nusing GeometricOptimizers\nusing GeometricOptimizers: 𝔄\nimport Random\nRandom.seed!(123)\n\nB = rand(StiefelLieAlgHorMatrix, 10, 2)\nB̂ = hcat(vcat(.5 * B.A, B.B), vcat(one(B.A), zero(B.B)))\nB̄ = hcat(vcat(one(B.A), zero(B.B)), vcat(-.5 * B.A, -B.B))\n\none(B̂ * B̄') + B̂ * 𝔄(B̂, B̄) * B̄' ≈ exp(Matrix(B))\n\n# output\n\ntrue\n\n\n\n\n\n","category":"method"},{"location":"#GeometricOptimizers.𝔄-Tuple{AbstractMatrix}","page":"Home","title":"GeometricOptimizers.𝔄","text":"𝔄(A)\n\nCompute mathfrakA(A) = sum_n=1^infty frac1n (A)^n-1\n\nImplementation\n\nThis uses a Taylor expansion that iteratively adds terms with\n\nwhile norm(Aⁿ) > ε\nmul!(A_temp, Aⁿ, A)\nAⁿ .= A_temp\nrmul!(Aⁿ, T(inv(n)))\n\n𝔄A += Aⁿ\nn += 1 \nend\n\nuntil the norm of Aⁿ becomes smaller than machine precision.  The counter n in the above algorithm is initialized as 2 The matrices Aⁿ and 𝔄 are initialized as the identity matrix.\n\n\n\n\n\n","category":"method"}]
}
